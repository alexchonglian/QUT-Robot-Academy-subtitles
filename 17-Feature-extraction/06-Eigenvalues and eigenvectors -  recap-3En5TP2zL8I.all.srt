1
00:00:03,360 --> 00:00:08,099
在这个可选部分中，我想要做的是重新审视特征值和特征值的概念
In this optional section what I want to do is revisit the concept of eigenvalues and

2
00:00:08,099 --> 00:00:12,509
特征向量，如果你忘记了它们是什么。它们是我们的概念
eigenvectors, in case you’ve forgotten what they are. They are a concept that we’re

3
00:00:12,509 --> 00:00:18,730
在下一部分需要。那么让我们考虑一个简单的线性代数
going to need in the next segment. So let’s consider a simple linear algebra

4
00:00:18,730 --> 00:00:24,540
案件。我们有一个向量，v。它是一个Nx1向量，我们要将它预先乘以a
case. We have a vector, v. It’s an Nx1 vector, and we’re going to pre-multiply it by a

5
00:00:24,540 --> 00:00:30,210
方阵矩阵NxN，结果当然是矢量，Nx1矢量。
square matrix NxN, and the result will of course be a vector, an Nx1 vector.

6
00:00:30,210 --> 00:00:37,300
现在对于每个矩阵'A'，有N个向量，其方向不变A.
Now for every matrix ‘A’, there are N vectors whose direction is unchanged by A.

7
00:00:37,300 --> 00:00:43,300
这意味着向量v和向量vprime将彼此平行。还有这些
That means the vector v and the vector v prime will be parallel to each other. And these

8
00:00:43,300 --> 00:00:48,520
我们称之为特征向量的向量。现在，虽然矢量v和矢量
vectors we refer to as eigenvectors. Now, although the vector v and the vector

9
00:00:48,520 --> 00:00:54,070
vprime将是平行的，它们可能不一定是相同的长度。所以长度
v prime will be parallel, they may not be necessarily the same length. So the length

10
00:00:54,070 --> 00:00:58,710
通过对应于特征向量的特征值来缩放。
is scaled by the eigenvalue that corresponds to the eigenvector.

11
00:00:58,710 --> 00:01:03,730
这就是很多话。让我们看一下实际中的图形示例。
So that’s a lot of words. Let’s look at a graphical example of this in practice.

12
00:01:03,730 --> 00:01:09,260
我们选择了一个非常简单的A矩阵;它是一个2乘2的矩阵，因此会有两个特征向量
We’ve chosen a very simple A matrix; it’s a 2 by 2 matrix, so there will be two eigenvectors

13
00:01:09,260 --> 00:01:14,080
和特征值。你可以看到一个特征值即将到来......现在。那就是
and eigenvalues. And you can see an eigenvalue just about coming up … now. That’s the

14
00:01:14,080 --> 00:01:19,330
红色矢量，输入矢量和蓝色输出矢量平行的情况
case where the red vector, the input vector, and the blue output vector are parallel to

15
00:01:19,330 --> 00:01:24,180
另一个。还有一个......现在。请注意，它们彼此平行
one another. There’s another one … now. Notice that they are parallel to each other

16
00:01:24,180 --> 00:01:28,260
但它们的长度不一样。所以红色矢量的长度与
but they’re not the same length. So the ratio of the length of the red vector to the

17
00:01:28,260 --> 00:01:34,530
蓝色矢量是特征值。所以术语特征向量......它是德国人
blue vector is the eigenvalue. So the term eigenvector … it’s a German

18
00:01:34,530 --> 00:01:39,180
字和它意味着'自己的矢量'。它是属于矩阵的特殊向量。
word and it means ‘own vector’. It’s a special vector that belongs to the matrix.

19
00:01:39,180 --> 00:01:43,290
所以我们可以编写几个方程式。首先，我们可以使用这个等式
So there are a couple of equations that we can write. First of all we can use the equation

20
00:01:43,290 --> 00:01:49,790
我们之前有过：v乘以矩阵A给出了输出向量vprime。
that we had before: that v multiplied by the matrix A gives us the output vector v prime.

21
00:01:49,790 --> 00:01:54,320
在特殊情况下，当它们平行时，它们就会被标量联系起来;
And in the particular case when they’re parallel then they are related by a scalar;

22
00:01:54,320 --> 00:01:58,270
这两个矢量是平行的，只是它们的长度不同。所以我们可以代表那个
that the two vectors are parallel, only their length is different. So we can represent that

23
00:01:58,270 --> 00:02:05,049
通过标量因子lambda，它是一个特征值。因此，如果这两个方程式为真，我们就可以
by the scalar factor lambda, which is an eigenvalue. So if these two equations are true we can

24
00:02:05,049 --> 00:02:09,090
写下这个，我们正在通过一些数学。我们对此不感兴趣
write this, and we’re following through some maths. We’re not interested in the

25
00:02:09,090 --> 00:02:14,080
在v=0的情况下，我们对A减去lambda的行列式的情况感兴趣，
case where v=0, we’re interested in the case where the determinant of A minus lambda,

26
00:02:14,080 --> 00:02:19,680
I，等于0.因此，如果我们可以为任何矩阵编写该表达式，那么它很容易
I, is equal to 0. So if we can write that expression for any matrix, it’s easy for

27
00:02:19,680 --> 00:02:24,500
一个2乘2的情况，3乘3更难，对于更大的矩阵非常非常困难，
a 2 by 2 case, more difficult for 3 by 3, very, very difficult for bigger matrices,

28
00:02:24,500 --> 00:02:29,930
我们可以解决特征值的问题;这些lambda值。
and we can solve that for the values of the eigenvalues; these lambda values.

29
00:02:29,930 --> 00:02:35,150
一旦我们得到了lambda的值，我们可以将它们替换为上面的等式，然后求解
Once we have values for lambda we can substitute them into the equations above and then solve

30
00:02:35,150 --> 00:02:41,060
对于特征向量。现在有无数个特征向量
for the eigenvector. Now there are an infinite number of eigenvectors

31
00:02:41,060 --> 00:02:45,910
对应于每个特征值，因此通常我们选择具有单位的特征向量
corresponding to each eigenvalue, so typically we choose eigenvectors that have got a unit

32
00:02:45,910 --> 00:02:50,980
长度。这是一个额外的约束，可以帮助我们选择一个特征向量。
length. That’s an additional constraint that helps us choose just a single eigenvector.

33
00:02:50,980 --> 00:02:56,160
我们将用一个简单的例子来证明这一点：使用2乘2矩阵。
We’ll demonstrate this with a simple example: with a 2 by 2 matrix.

34
00:03:01,940 --> 00:03:06,900
该矩阵的特征值仅使用MATLAB内置函数eig给出。我们
The eigenvalues of this matrix are given simply using the MATLAB built-in function eig. We

35
00:03:06,910 --> 00:03:12,480
看到两个特征值是1和-2。找到特征向量更多一点
see the two eigenvalues are 1 and -2. To find the eigenvectors is a little bit more

36
00:03:12,480 --> 00:03:21,680
复杂。我们必须从函数eig和第一个输出中分配两个输出参数
complex. We have to assign two output arguments from the function eig, and the first output

37
00:03:21,680 --> 00:03:26,190
我已经分配给矩阵x的参数有两列。第一列是
argument which I’ve assigned to the matrix x has got two columns. The first column is

38
00:03:26,190 --> 00:03:31,060
第一个特征向量。第二列是第二个特征向量。和特征值
the first eigenvector. The second column is the second eigenvector. And the eigenvalues

39
00:03:31,060 --> 00:03:37,010
现在沿着输出矩阵e的对角线排列。
now are arranged along the diagonal of the output matrix e.

40
00:03:37,010 --> 00:03:42,489
特征向量的一个基本属性是当它被原始变换时
A fundamental property of the eigenvector is that when it is transformed by the original

41
00:03:42,489 --> 00:03:48,159
矩阵，它与自身平行。我们来试试吧。
matrix, it is parallel to itself. Let’s test that.

42
00:03:48,159 --> 00:03:54,090
原始矩阵A，我们将它乘以第一个特征向量
The original matrix A, and we multiply it by the first eigenvector which is the first

43
00:03:54,090 --> 00:04:03,170
矩阵x的列。我们看到结果向量确实与特征向量平行，
column of the matrix x. We see that the resulting vector is indeed parallel to the eigenvector,

44
00:04:03,170 --> 00:04:10,280
它具有完全相同的幅度。它被缩放为1，这就是它的值
and it has exactly the same magnitude. It’s been scaled by 1, and that is the value of

45
00:04:10,280 --> 00:04:18,560
第一个特征值。因此，乘以矩阵A的特征向量是平行于矩阵A的向量
the first eigenvalue. So the eigenvector multiplied by the matrix A is a vector parallel to the

46
00:04:18,560 --> 00:04:22,770
比例因子为1的特征向量。事实上，它是原始的特征向量。
eigenvector with a scale factor of 1. So it is, in fact, the original eigenvector.

47
00:04:22,770 --> 00:04:28,830
让我们用另一个特征向量来尝试，这是矩阵x的第二列。
Let’s try it with the other eigenvector, which is the second column of the matrix x.

48
00:04:28,830 --> 00:04:35,069
我们现在看到结果确实与第二个特征向量平行，但它已经存在
We see now that the result is indeed parallel to the second eigenvector, but it’s been

49
00:04:35,069 --> 00:04:41,919
按-2缩放，-2是矩阵A的第二个特征值。
scaled by -2, and -2 is the second eigenvalue of the matrix A.

